#Evaluation#
Through the process of completing this project, a high level overview can be taken and examined, allowing a full evaluation of this project.
Firstly, it is worth looking at what has been achieved in the project, with respect to both the original specification and the software as a whole. When looking back at the specification, it has been shown (please refer to the test report) that all of the functional requirements have been satisfied. This means the final software produced achieves at least what was initially set. In addition to this, working ahead of schedule has allowed the addition of originally unspecified features, such as the replaying of instructions in the CPU visualisation.

Other added features include the addition of windows for labels for example, but also the extensive help windows and references available from within the system. These allow the user to retrieve information on what each register should be used for, the instructions available for use within the simulator as well as information on the system as a whole.

From a design perspective there is one aspect of the system that has worked extremely well and allows an incredible amount of extensibility from the system. This is the generality with respect to the algorithms that can be run and visualised.

From personal experience of adding new algorithms to the system, it has proven very straightforward with no consideration of the internals of the system required. New algorithms can be written via the writing of the MIPS code for it; the annotation language then allows easy integration with visual components through straightforward methods for operations such as swapping, emphasis etc. If a visual component already exists that can be used for visualising an algorithm, then that is the extent of what is required. If a new visualisation needs to be made, that is all that has to be done: it can be written in JavaFX and then used. This has given the system the aforementioned generality by giving the potential to add visualisations for any algorithm desired to be included within the system. Consequently, this had led to the production of a highly flexible system.

As well as the achievement of the functional requirements, there were also the non-functional requirements to be satisfied. The easiest method of testing said requirements is through the delivery of the software to the users. Although this is covered in detail in the test report, the users' opinions on the software are crucial to the evaluation of the system. Two of the biggest (non-functional) aims of this project was to improve upon the existing software (Spim Simulator) and to also provide a system which was helpful to the students in their understanding of the content of the Computer Systems & Architecture module. When asked both of these questions explicitly, the large majority of students said it was preferable over Spim Simulator and also that it would prove helpful. Additionally, when finding users' favourite visualisation, the results came back with a reasonably even distribution, suggesting that, in general, all of the visualisations provided are proving as useful as each other. In general positive user response signals that performance/usability requirements have been met. Any other non-functional requirements were met simply, by keeping to them, for example, not allowing local file access through the JavaScript annotation language.

Another aspect of the system which has proven successful is the plan for its development. At no point in this project has time been a major issue, and for the most part, it could be seen that components/features of the system were being delivered on time. Although this required a fairly dense work rate (arguably any project being conducted in this timescale would require this), the plan was well organised such that the completion of work was possible, with minimal bottlenecks within the development plan. It is for this reason that Simulizer was able to contain its large array of functionality.

The previous points have discussed what has been achieved with the system, and how the system has been a success with particular regards to the requirements set out in the specification document. However, to provide a balanced evaluation of the system, areas for improvement will be discussed. Possibly one of the biggest occurrences of this in the system was a possible underestimation of the precision required to get a large multi-threaded system correct. In particular, earlier on during the development of the system, less attention may have been given to preventing concurrency errors, such as race conditions and deadlock. As a result, significant amounts of time nearer the end of the project was spent fixing said concurrency errors, and in some cases, certain methods of threading had to be redesigned such that they were more stable and less error prone.

One more possible improvement that may have proven useful is all team members having more knowledge of the whole code base. With no doubt, the separation of work between team members was very effective in maintaining a well-designed system while still providing an efficient workflow. However, later on in development, when most of the threading issues started to appear, certain team members were an unable to provide much aid in fixing these errors due to their work being separate from the threading design. If every team member had an understanding of the whole system, then this may have aided in the fixing of bugs later on. However, this still had an upside that it enabled certain members to focus on these areas, while other team members worked on other important areas of the project.

Reaching the end of the project, there are two defining aspects to be discussed, which measure the team's success over the last 10 weeks. The first of these is the goal of Simulizer being used as a resource in the Computer Systems & Architecture module in the following years. After discussions/demonstrations of the system with the module leader (Ian Batten), there is a real chance of it being used. He is planning to take a copy of the software once the project timeline is over to take and use to see whether it is suitable. As a result, this may entail further additions/changes being made to Simulizer after the project is over. This represents a significant achievement for the team and for the software, and so this is something the team is more than willing to continue with.

In addition to this, there is further potential for Simulizer. After discussions with the team's tutor and Ian Batten, it has been mentioned that there is genuine innovation within Simulizer, giving it the potential to be used as a teaching resource. Included in this are aspects of the system such as an easy to understand visualisation of a pipeline (something which had previously been described by Ian Batten as difficult to teach). In addition, the JavaScript annotation language is an effective method of debugging assembly programs at a high level and allows for fine grain control over the visualisations displayed on screen as well as allowing explicit changes to the simulator (such as register changes for example). It is for this reason that, potentially, a paper could be written discussing the innovation Simulizer is providing as a teaching and learning resource. Given the teams current stage into their respective academic careers, and the relatively short time scale for the project, this would be a great accomplishment.

As a balanced whole, Simulizer can be considered a successful project. It has achieved everything that was originally set to be accomplished (and more so in some cases) and it has been well received by the user base (both sides of it as well). There are some gaps for potential improvement, but given the environment in which Simulizer has been developed, this is almost inevitable.
