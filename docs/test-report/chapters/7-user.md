# User testing #

As well as providing very objective sets of tests for proving the correctness of our software and also that the functionality outlined in the specification is present in the software, there are also a base of users, from whom the software has to be accepted. In order to do this, once the stage was met where significant amounts of the intended functionality had been written, the software was released to our user base (via a website which was linked to the module Facebook page; this gave an easy way of communicating with users). Along with the software, a brief guide on using Simulizer was given, alongside a list of tasks suggested to carry out in order to test the system. However, there were no restrictions on this and the users were given free reign over the system. They were then asked to complete a questionnaire on how they found their experience with Simulizer. As well as the students, Ian Batten was also asked to test the system. The responses of the user testing questionnaire are as follows:

![User Response Bar Charts](segments/userTests/bars.png){width=80%}

![User Response Pie Charts](segments/userTests/pies.png){width=80%}

In addition to these discrete questions, users were also asked for written answers to certain questions. This method of feedback helped significantly in the further detection of more subtle bugs. As an example, through the use of user testing it was found that in certain situations, the text editor was remaining read only, even after the ending of the simulation. Having this detected by the user allowed us to find the cause of the error and consequently fix it.

The users also provided the team with suggestions in which the system could be improved, such as giving a register reference and explaining the colours of the line highlighting. These were all very valuable comments to receive as it showed our users had an active engagement with Simulizer. Whenever one of these such comments was received, the feature being requested (or needing to be fixed) would try to be implemented in the most part. This has only improved the usability of Simulizer.

Finally, Simulizer received a large amount of praise. For example, as seen in the pie charts above, 90% of students claimed they would find Simulizer useful for the Computer Systems & Architecture module. As well as this, 80% of students said they preferred to Spim Simulator which is a very proud feat for the team. Some of the positive written feedback includes: "Friendly looking UI, lots of useful visualisations. 8/8 would recommend", "Overall very nice appearance and usable" and "For me it was not a matter of understanding, but a matter of usability. Writing code and running it in SPIM is a tedious and somewhat confusing, error-prone task. This is a far superior environment which I would actually feel comfortable writing large MIPS programs in.". These comments give very high praise indeed, and this signalled to the team that the project was heading towards the right direction. As a whole, the user testing has proved invaluable to the success of this project. For a more in depth look at the written feedback, please refer to the appendix of this document.
